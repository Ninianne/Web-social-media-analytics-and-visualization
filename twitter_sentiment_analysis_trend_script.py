# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis_Trend script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13GvM4FHjSy-mD_2hBQ2DveiIYLYRybsF
"""

import tweepy as tw
import pandas as pd
from matplotlib import pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import time

# App Auth
consumer_key='xxxxxxxxxxxxxxxx'
consumer_secret_key='xxxxxxxxxxxxxxxxx'
access_token='xxxxxxxxxxxxxxxxxxxx'
access_token_secret='xxxxxxxxxxxxxxxx'

# Initialize API
auth = tw.OAuthHandler(consumer_key, consumer_secret_key)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)

# Search terms
search_words = "police  -filter:retweets", "#KillTheBill -filter:retweets"
date_since = "2021-03-01"
date_until ="2021-04-06"

# Collect tweets
tweets = tw.Cursor(api.search,
              q=search_words,
              lang="en",
              since=date_since,
              result_type="mixed",
              until=date_until).items(2500)

# Iterate tweets
try:
	tweet_details = [[tweet.text,tweet.user.screen_name,tweet.source,tweet.user.location,tweet.user.verified,tweet.created_at] for tweet in tweets]
except tweepy.TweepError as e:
 	print(e.reason)
time.sleep(60)

#Store tweet in a dataframe
tweet_df = pd.DataFrame(data=tweet_details, columns=['text','user','source', "location","verified", "created_at"])

# NLP imports
import nltk
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

#python imports
import string
import re

# preprocessor data sources and instances
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words("english"))
lm = WordNetLemmatizer()

#define functions to clean tweet
def clean_tweets(text):
     # remove mentions, hashtags and punctuation
    text = re.sub(r'RT|@[a-zA-Z0-9_]+|:|#[a-zA-Z0-9_]+|[0-9]+|,|\'|\"|\`|â€¦', '', text)
    text = re.sub(r'  ', '', text.strip())
    #remove Http sign
    text = re.sub("https?://[A-Za-z0-9./]*","",text)
    text = re.sub("\n","",text)
    text = re.sub("RT","",text)
    return text
def clean_tweets(source):
    source = re.sub("Twitter for","",source)
    source = re.sub("Twitter","",source)
    return source

tweet_df['text']=tweet_df['text'].apply(lambda x: clean_tweets(x))
tweet_df['source']=tweet_df['source'].apply(lambda x: clean_tweets(x))

#Lemmatization
sentences_processed = []
for sentence in tweet_df['text']:
    # Obtain word tokens
    tokenized_words = word_tokenize(sentence)
    # Remove stop words
    filtered_words = list(filter(lambda x: x not in stop_words, tokenized_words))
    # Lemmatization
    lemmatized_words = list(map(lm.lemmatize, filtered_words))
    sentences_processed.append(" ".join(lemmatized_words))

data = pd.DataFrame(data={'text': sentences_processed})

#update dataframe
tweet_df['text'] = data['text']
tweet_df.head(20)
#len(tweet_df)

text = tweet_df.location.value_counts().head(20)
wordcloud = WordCloud(background_color='white',mode="RGB", width=1000 , height=500).generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

#Top 6 tweet sources
tweet_source = tweet_df.source.value_counts()

explode = (0.2, 0.1, 0.1, 0.1, 0.1,0.1,0.1,0.1,0.1,0.1) 
df2 = tweet_source[:6].plot(kind = 'pie', autopct='%1.0f%%', pctdistance=1.1, labeldistance=1.2, radius=2)

verification = tweet_df.verified.value_counts()

explode = (0.2, 0.1, 0.1, 0.1, 0.1,0.1,0.1,0.1,0.1,0.1) 
df3 = verification[:10].plot(kind = 'pie', autopct='%1.0f%%', pctdistance=1.1, labeldistance=1.2, radius=2)

#filter tweet by location

uk_tweets = tweet_df[tweet_df["location"].str.contains("United Kingdom|London|England, Europe| Uk", na=False)]
uk_tweets.head(20)

#implementing LDA
import sklearn;
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer;
from sklearn.decomposition import LatentDirichletAllocation

# LDA is able to use tf-idf
no_features = 5000
tfidf_vectorizer = TfidfVectorizer(max_df=0.50, min_df=1, max_features=no_features, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(uk_tweets['text'])
tfidf_feature_names = tfidf_vectorizer.get_feature_names()

#Initialize the number of Topics we need to cluster:
num_topics = 20;

lda = LatentDirichletAllocation(n_components=num_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf)

tfidf

#display topics
def display_topics(model, feature_names, no_top_words):
 for topic_idx, topic in enumerate(model.components_):
  print ("Topic", topic_idx)
  print (" ".join([feature_names[i]
    for i in topic.argsort()[:-no_top_words - 1:-1]]))

no_top_words = 15
display_topics(lda, tfidf_feature_names, no_top_words)

#load nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

analyser = SentimentIntensityAnalyzer()

#loop through tweet and assign sentiment score
i=0

#empty list to hold computed 'compound' VADER scores
compval1 = [ ]


while (i<len(uk_tweets)):

    k = analyser.polarity_scores(uk_tweets.iloc[i]['text'])
    compval1.append(k['compound'])
    
    i = i+1

len(compval1)

len(uk_tweets)

#display score
uk_tweets['VADER score'] = compval1
uk_tweets.head(20)

#Assigning score sentiment categories
i = 0

predicted_value = [ ] #empty series to hold our predicted values

while(i<len(uk_tweets)):
    if ((uk_tweets.iloc[i]['VADER score'] >= 0.7)):
        predicted_value.append('positive')
        i = i+1
    elif ((uk_tweets.iloc[i]['VADER score'] > 0) & (uk_tweets.iloc[i]['VADER score'] < 0.7)):
        predicted_value.append('neutral')
        i = i+1
    elif ((uk_tweets.iloc[i]['VADER score'] <= 0)):
        predicted_value.append('negative')
        i = i+1

uk_tweets['sentiment'] = predicted_value
uk_tweets.head(20)

#Plot bar chart showing the sentiment levels
uk_tweets.groupby('sentiment').size().plot(kind='bar')

#word cloud of most frequenct words
words = ' '.join(uk_tweets['text'])
cleaned_word = " ".join([word for word in words.split()
                            if 'http' not in word
                                and not word.startswith('@')
                                and word != 'RT'
                            ])
stopwords = set(STOPWORDS)
stopwords.add("Simon_Vessey")
stopwords.add("showed")
stopwords.add("panny_antoniou")
stopwords.add("n't")
stopwords.add("even")
stopwords.add("overhead")
stopwords.add("Y5YZMhYEMm")
wordcloud = WordCloud(stopwords=stopwords,
                      background_color='white',
                      max_words=100,
                      width=1500,
                      height=1250
                     ).generate(cleaned_word)

plt.figure(1,figsize=(12, 12))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()